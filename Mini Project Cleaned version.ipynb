{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03630725",
   "metadata": {},
   "source": [
    "<center> <span style=\"font-size:30px;\"><span style=\"color:black;\"><b>SC1015 Mini Project</b></span></span> </center>\n",
    "<br>\n",
    "<center> <span style=\"font-size:30px;\"><span style=\"color:black;\"><b>Heart Disease Predictor</b></span></span> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f4bd7",
   "metadata": {},
   "source": [
    "# Importing of Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "sb.set() # set the default Seaborn style for graphics\n",
    "import plotly_express as px\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score , ConfusionMatrixDisplay , classification_report\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c9771",
   "metadata": {},
   "source": [
    "# 1. Analyse of Data\n",
    "For our mini-project we will be using the FramingHam data set from Kaggle. The aim of this project is to predict if a patient has a 10 years risk of future (CHD) coronary heart disease.\n",
    "<br>\n",
    "We will begin by importing the data and going through the variables in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab37ac",
   "metadata": {},
   "source": [
    "heartData = pd.read_csv('framingham.csv')\n",
    "heartData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the Variables\n",
    "heartData.info()\n",
    "heartData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869ba90",
   "metadata": {},
   "source": [
    "Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d2d20",
   "metadata": {},
   "source": [
    "# 2. Cleaning/Preprocessing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f45878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc56b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0314f2e1",
   "metadata": {},
   "source": [
    "# 3. Exploratory Analysis\n",
    "Now we have a processed data set, we will start by finding out which variables play a strong factor in determining the final prediction result.\n",
    "<br>\n",
    "First lets explore each variables by generating various graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60c9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239afbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a5b41d6",
   "metadata": {},
   "source": [
    "We will generate a correlation matrix to evaluate the relationship between every variables in the data set by looking at its correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da957ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = heartData.drop(columns= 'TenYearCHD').corr()\n",
    "fig , ax = plt.subplots(figsize=(25 , 20))\n",
    "sb.heatmap(corr ,annot= True , ax=ax , cmap= 'Greens');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b6651",
   "metadata": {},
   "source": [
    "Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973cbb5d",
   "metadata": {},
   "source": [
    "# 4. Exploring of Machine Learning Techniques\n",
    "Now we explored the variables, we will be trying out differen machine learning techniques to see which methods gives the best accuracy and using the confusion matrix to try out the prediction using test values.\n",
    "<br>\n",
    "We will be predicting TenYearCHD using the rest of variables. TenYearCHD is a boolean data type consisting of 0 and 1 (True and False), so machine learning techniques catering to classification will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e95369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing additional libraries for graph plotting of model results\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heartData.drop('TenYearCHD' , axis= 'columns')\n",
    "y = heartData['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c71f80",
   "metadata": {},
   "source": [
    "Create appropriate datasets for Train and Test in an 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a40f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2 , shuffle=True , random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2076a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13d48f",
   "metadata": {},
   "source": [
    "## Baseline Classifer\n",
    "A baseline classifier will be used to provide a simple, minimalistic model that serves as a reference point for evaluating the performance of more complex models. This establish a performance benchmark when we make our comparison with other models and our final justification in picking the best machine learning technique to generate the final model for our predicition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181335d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline classifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "dummy_classifier.fit(X_train , y_train)\n",
    "y_pred = dummy_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test , y_pred)\n",
    "print(f\"Baseline Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11279edf",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "A regression analysis describe data and explain relationship between 1 binary variable and one or more nominal, ordinal or interval independent variables. The outcome from this analysis will give a 'Yes' or 'No' result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f53b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model\n",
    "\n",
    "lr_model = make_pipeline(SimpleImputer(strategy='mean') , MinMaxScaler() , LogisticRegression(penalty='l2' , C= 12 ,max_iter=1500))\n",
    "lr_model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_acc_score = accuracy_score(y_test , lr_pred)\n",
    "lr_acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed2745",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree model\n",
    "\n",
    "dt_model = make_pipeline(SimpleImputer(strategy='mean') , MinMaxScaler() , DecisionTreeClassifier())\n",
    "dt_model.fit(X_train , y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dt_model.predict(X_test)\n",
    "dt_acc_score = accuracy_score(y_test , dt_pred)\n",
    "dt_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfecfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(dt_model , X_test , y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b06eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d150e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7411ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
