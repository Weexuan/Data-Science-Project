{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03630725",
   "metadata": {},
   "source": [
    "<center> <span style=\"font-size:30px;\"><span style=\"color:black;\"><b>SC1015 Mini Project</b></span></span> </center>\n",
    "<br>\n",
    "<center> <span style=\"font-size:30px;\"><span style=\"color:black;\"><b>Heart Disease Predictor</b></span></span> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f4bd7",
   "metadata": {},
   "source": [
    "# Importing of Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly-express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "sb.set() # set the default Seaborn style for graphics\n",
    "import plotly_express as px\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score , ConfusionMatrixDisplay , classification_report, confusion_matrix\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c9771",
   "metadata": {},
   "source": [
    "# 1. Analyse of Data\n",
    "For our mini-project we will be using the FramingHam data set from Kaggle. The aim of this project is to predict if a patient has a 10 years risk of future (CHD) coronary heart disease.\n",
    "<br>\n",
    "We will begin by importing the data and going through the variables in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee6793-51c1-4514-8bc2-4e2f64c2af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartData = pd.read_csv('framingham.csv')\n",
    "heartData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the Variables\n",
    "heartData.info()\n",
    "heartData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2af8f",
   "metadata": {},
   "source": [
    "Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d2d20",
   "metadata": {},
   "source": [
    "# 2. Cleaning/Preprocessing of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c74f4",
   "metadata": {},
   "source": [
    "## Missing Value Cleaning\n",
    "Check to see if there is any null values in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of null values in each column\n",
    "(heartData.isnull().sum()/heartData.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f093f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartData['education'].fillna(0,inplace=True)\n",
    "heartData['cigsPerDay'].fillna(heartData['cigsPerDay'].where(heartData['currentSmoker']==1).median(),inplace=True)\n",
    "heartData['BPMeds'].fillna(0,inplace=True)\n",
    "heartData['totChol'].fillna(heartData['totChol'].median(),inplace=True)\n",
    "heartData['BMI'].fillna(heartData['BMI'].median(),inplace=True)\n",
    "heartData['heartRate'].fillna(heartData['heartRate'].where(heartData['currentSmoker']==1).median(),inplace=True)\n",
    "heartData['glucose'].fillna(heartData['glucose'].where(heartData['diabetes']==0).median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26155fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any misisng values:\n",
    "(heartData.isnull().sum()/heartData.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0792fb",
   "metadata": {},
   "source": [
    "## Outlier cleaning\n",
    "Identify which variables needs outliers cleaning and remove the outlier rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646255b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backing up data sets for any just in case\n",
    "heartData_Copy = heartData.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the distributions of all variables\n",
    "f, axes = plt.subplots(16, 2, figsize=(20,75))\n",
    "\n",
    "count = 0\n",
    "for var in heartData:\n",
    "    sb.boxplot(data = heartData[var], orient = \"h\", ax = axes[count,0])\n",
    "    axes[count,0].set_title(f'Boxplot of {var}', color='DarkRed')  \n",
    "    sb.violinplot(data = heartData[var], orient = \"h\", ax = axes[count,1])\n",
    "    axes[count,1].set_title(f'Violinplot of {var}', color='DarkRed') \n",
    "    count += 1\n",
    "    \n",
    "#Calculate outliers\n",
    "Q1 = heartData.quantile(0.25)\n",
    "Q3 = heartData.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print (((heartData < (Q1 - 1.5 * IQR)) | (heartData > (Q3 + 1.5 * IQR))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc56b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting variables that requires outlier remove\n",
    "outlier_variables = ['cigsPerDay','totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove outliers\n",
    "for var in outlier_variables:\n",
    "    Q1 = heartData[var].quantile(0.25)\n",
    "    Q3 = heartData[var].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    UL = Q3 + 1.5 * IQR\n",
    "    LL = Q1 - 1.5 * IQR\n",
    "    heartData = heartData[(heartData[var] < UL) & (heartData[var] > LL)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There were {} rows before outlier treatment.'.format(heartData_Copy.shape[0]))\n",
    "print('There are {} rows after outlier treatment.'.format(heartData.shape[0]))\n",
    "print('After outlier treatment number of rows lost are {}.'.format(heartData_Copy.shape[0] - heartData.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314f2e1",
   "metadata": {},
   "source": [
    "# 3. Exploratory Analysis\n",
    "Now we have a processed data set, we will start by finding out which variables play a strong factor in determining the final prediction result.\n",
    "<br>\n",
    "First lets explore each variables by generating various graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bdd6bc-c47d-44b9-b465-8dcad4145424",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histograms(dataframe, features, rows, cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, feature in enumerate(features):\n",
    "        ax=fig.add_subplot(rows,cols,i+1)\n",
    "        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n",
    "        ax.set_title(feature+\" Distribution\",color='DarkRed')\n",
    "        \n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "draw_histograms(heartData,heartData.columns,6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartData.TenYearCHD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975e419-faee-43fb-b0ef-ae30a2325f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.countplot(x='TenYearCHD',data=heartData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ced2e5",
   "metadata": {},
   "source": [
    "There are 3594 patients with no heart disease and 644 patients with risk of heart disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575a9ee-9c0d-43a9-9833-0c0b925f5ca3",
   "metadata": {},
   "source": [
    "We will generate a correlation matrix to evaluate the relationship between every variables in the data set by looking at its correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da957ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = heartData.corr()\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "fig , ax = plt.subplots(figsize=(25 , 20))\n",
    "sb.heatmap(correlation_matrix, annot=True, cmap='Greens', ax=ax)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c095c-7c55-425b-b3c8-cfaeba56c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of males and females with TenYearCHD\n",
    "male_chd_percentage = heartData[heartData['male'] == 1]['TenYearCHD'].mean() * 100\n",
    "female_chd_percentage = heartData[heartData['male'] == 0]['TenYearCHD'].mean() * 100\n",
    "\n",
    "# Plotting\n",
    "plt.bar(['Male', 'Female'], [male_chd_percentage, female_chd_percentage], color=['blue', 'pink'])\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccc8bf-052f-44a9-a572-5c8a04441790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of individuals in different age groups with TenYearCHD\n",
    "age_groups = [30, 40, 50, 60, 70]\n",
    "age_chd_percentage = []\n",
    "\n",
    "for age in age_groups:\n",
    "    chd_percentage = heartData[(heartData['age'] >= age) & (heartData['age'] < age + 10)]['TenYearCHD'].mean() * 100\n",
    "    age_chd_percentage.append(chd_percentage)\n",
    "\n",
    "# Plotting\n",
    "plt.bar(age_groups, age_chd_percentage, color='green')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Age Group')\n",
    "plt.xticks(age_groups)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0d3bb-ad17-40f3-b4ef-10685b6736d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of current smokers and non-smokers with TenYearCHD\n",
    "current_smoker_chd_percentage = heartData[heartData['currentSmoker'] == 1]['TenYearCHD'].mean() * 100\n",
    "non_smoker_chd_percentage = heartData[heartData['currentSmoker'] == 0]['TenYearCHD'].mean() * 100\n",
    "\n",
    "# Plotting\n",
    "plt.bar(['Current Smoker', 'Non-Smoker'], [current_smoker_chd_percentage, non_smoker_chd_percentage], color=['orange', 'purple'])\n",
    "plt.xlabel('Smoking Status')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Smoking Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092e6df-2c6f-44bd-97f7-6d74aaa9cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of individuals in different cigarette per day groups with TenYearCHD\n",
    "cigs_per_day_groups = [0, 10, 20, 30, 40, 50,60]\n",
    "cigs_per_day_chd_percentage = []\n",
    "\n",
    "for cigs_per_day in cigs_per_day_groups:\n",
    "    chd_percentage = heartData[(heartData['cigsPerDay'] >= cigs_per_day) & (heartData['cigsPerDay'] < cigs_per_day + 10)]['TenYearCHD'].mean() * 100\n",
    "    cigs_per_day_chd_percentage.append(chd_percentage)\n",
    "\n",
    "# Plotting\n",
    "plt.bar(cigs_per_day_groups, cigs_per_day_chd_percentage, color='red')\n",
    "plt.xlabel('Cigarettes per Day')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Cigarettes per Day Group')\n",
    "plt.xticks(cigs_per_day_groups)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7faee1-d355-4b45-bb06-6f93003dfd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_groups = [0, 18.5, 25, 30, 35, 40, float('inf')]\n",
    "bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese Class I', 'Obese Class II', 'Obese Class III']\n",
    "bmi_chd_percentage = []\n",
    "\n",
    "for i in range(len(bmi_groups) - 1):\n",
    "    lower_bound = bmi_groups[i]\n",
    "    upper_bound = bmi_groups[i + 1]\n",
    "    chd_percentage = heartData[(heartData['BMI'] >= lower_bound) & (heartData['BMI'] < upper_bound)]['TenYearCHD'].mean() * 100\n",
    "    bmi_chd_percentage.append(chd_percentage)\n",
    "\n",
    "# Plotting\n",
    "plt.bar(bmi_labels, bmi_chd_percentage, color='blue')\n",
    "plt.xlabel('BMI Categories')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by BMI Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631abf1-c35c-48b7-8ae8-17b81965781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_chd_percentage = heartData[heartData['diabetes'] == 1]['TenYearCHD'].mean() * 100\n",
    "no_diabetes_chd_percentage = heartData[heartData['diabetes'] == 0]['TenYearCHD'].mean() * 100\n",
    "\n",
    "# Plotting\n",
    "plt.bar(['Diabetes', 'No Diabetes'], [diabetes_chd_percentage, no_diabetes_chd_percentage], color=['orange', 'blue'])\n",
    "plt.xlabel('Diabetes Status')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Diabetes Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d8b97-cbc1-427c-ab45-5e0931a03e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of individuals in different glucose level groups with TenYearCHD\n",
    "glucose_groups = [0, 100, 125, 150, 200, float('inf')]\n",
    "glucose_labels = ['Normal', 'Prediabetes', 'Borderline Diabetes', 'Diabetes', 'Severe Diabetes']\n",
    "glucose_chd_percentage = []\n",
    "\n",
    "for i in range(len(glucose_groups) - 1):\n",
    "    lower_bound = glucose_groups[i]\n",
    "    upper_bound = glucose_groups[i + 1]\n",
    "    chd_percentage = heartData[(heartData['glucose'] >= lower_bound) & (heartData['glucose'] < upper_bound)]['TenYearCHD'].mean() * 100\n",
    "    glucose_chd_percentage.append(chd_percentage)\n",
    "\n",
    "# Plotting\n",
    "plt.bar(glucose_labels, glucose_chd_percentage, color='green')\n",
    "plt.xlabel('Glucose Level Categories')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Glucose Level Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e47725-4a0e-42d4-85cd-ab36f480a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "totChol_bins = [0, 200, 240, 280, float('inf')]\n",
    "totChol_labels = ['Desirable (0-200)', 'Borderline High (200-240)', 'High (240-280)', 'Very High (>280)']\n",
    "\n",
    "# Calculate percentage of TenYearCHD for each bin\n",
    "totChol_chd_percentage = []\n",
    "for i in range(len(totChol_bins) - 1):\n",
    "    lower_bound = totChol_bins[i]\n",
    "    upper_bound = totChol_bins[i + 1]\n",
    "    chd_percentage = heartData[(heartData['totChol'] >= lower_bound) & (heartData['totChol'] < upper_bound)]['TenYearCHD'].mean() * 100\n",
    "    totChol_chd_percentage.append(chd_percentage)\n",
    "\n",
    "# Plotting\n",
    "plt.bar(totChol_labels, totChol_chd_percentage, color='blue')\n",
    "plt.xlabel('Total Cholesterol (mg/dL)')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Total Cholesterol Level')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361c730-e582-4dc9-a350-94ab8500868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysBP_bins = [0, 120, 130, 140, 180, float('inf')]\n",
    "sysBP_labels = ['Normal (0-120)', 'Elevated (120-130)', 'Hypertension Stage 1 (130-140)', 'Hypertension Stage 2 (140-180)', 'Hypertensive Crisis (>180)']\n",
    "\n",
    "# Calculate percentage of TenYearCHD for each bin of sysBP\n",
    "sysBP_chd_percentage = []\n",
    "for i in range(len(sysBP_bins) - 1):\n",
    "    lower_bound = sysBP_bins[i]\n",
    "    upper_bound = sysBP_bins[i + 1]\n",
    "    chd_percentage = heartData[(heartData['sysBP'] >= lower_bound) & (heartData['sysBP'] < upper_bound)]['TenYearCHD'].mean() * 100\n",
    "    sysBP_chd_percentage.append(chd_percentage)\n",
    "\n",
    "# Plotting sysBP\n",
    "plt.bar(sysBP_labels, sysBP_chd_percentage, color='red')\n",
    "plt.xlabel('Systolic Blood Pressure (mmHg)')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Systolic Blood Pressure')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21948a5c-2098-4c15-897c-25a7ce7672c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins for Diastolic Blood Pressure (diaBP)\n",
    "diaBP_bins = [0, 80, 90, 120, float('inf')]\n",
    "diaBP_labels = ['Normal (0-80)', 'High-Normal (80-90)', 'Hypertension Stage 1 (90-120)', 'Hypertension Stage 2 (>120)']\n",
    "\n",
    "# Calculate percentage of TenYearCHD for each bin of diaBP\n",
    "diaBP_chd_percentage = []\n",
    "for i in range(len(diaBP_bins) - 1):\n",
    "    lower_bound = diaBP_bins[i]\n",
    "    upper_bound = diaBP_bins[i + 1]\n",
    "    chd_percentage = heartData[(heartData['diaBP'] >= lower_bound) & (heartData['diaBP'] < upper_bound)]['TenYearCHD'].mean() * 100\n",
    "    diaBP_chd_percentage.append(chd_percentage)\n",
    "\n",
    "# Plotting diaBP\n",
    "plt.bar(diaBP_labels, diaBP_chd_percentage, color='green')\n",
    "plt.xlabel('Diastolic Blood Pressure (mmHg)')\n",
    "plt.ylabel('Percentage with TenYearCHD')\n",
    "plt.title('Percentage of TenYearCHD by Diastolic Blood Pressure')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654c5b3",
   "metadata": {},
   "source": [
    "Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aec676",
   "metadata": {},
   "source": [
    "# 4. Exploring of Machine Learning Techniques\n",
    "Now we explored the variables, we will be trying out different machine learning techniques to see which methods gives the best accuracy and using the confusion matrix to try out the prediction using test values.\n",
    "<br>\n",
    "We will be predicting TenYearCHD using the rest of variables. TenYearCHD is a boolean data type consisting of 0 and 1 (True and False), so machine learning techniques catering to classification will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing additional libraries for graph plotting of model results\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heartData.drop('TenYearCHD' , axis= 'columns')\n",
    "y = heartData['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b53db",
   "metadata": {},
   "source": [
    "Create appropriate datasets for Train and Test in an 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2 , shuffle=True , random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd67c32",
   "metadata": {},
   "source": [
    "## Baseline Classifer\n",
    "A baseline classifier will be used to provide a simple, minimalistic model that serves as a reference point for evaluating the performance of more complex models. This establish a performance benchmark when we make our comparison with other models and our final justification in picking the best machine learning technique to generate the final model for our predicition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29383286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline classifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "dummy_classifier.fit(X_train , y_train)\n",
    "y_pred = dummy_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test , y_pred)\n",
    "print(f\"Baseline Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce384b3a",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "A regression analysis describe data and explain relationship between 1 binary variable and one or more nominal, ordinal or interval independent variables. The outcome from this analysis will give a 'Yes' or 'No' result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3637ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression model\n",
    "\n",
    "lr_model = make_pipeline(SimpleImputer(strategy='mean') , MinMaxScaler() , LogisticRegression(penalty='l2' , C= 12 ,max_iter=1500))\n",
    "lr_model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.score(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_acc_score = accuracy_score(y_test , lr_pred)\n",
    "lr_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients Plot\n",
    "coefficients = lr_model.named_steps['logisticregression'].coef_.flatten()\n",
    "feature_names = X_train.columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, coefficients)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Coefficients of Logistic Regression Model')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(roc_auc_score(y_test, y_pred_proba)))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label='Precision-Recall Curve (AP = {:.2f})'.format(average_precision_score(y_test, y_pred_proba)))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, lr_model.predict(X_test)), display_labels=[\"Class 0\", \"Class 1\"])\n",
    "confusion_matrix_display.plot()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(lr_model, X_train, y_train, cv=5)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score')\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Validation Score')\n",
    "plt.xlabel('Training Examples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out FPR TPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8bad4",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree model\n",
    "\n",
    "dt_model = make_pipeline(SimpleImputer(strategy='mean') , MinMaxScaler() , DecisionTreeClassifier())\n",
    "dt_model.fit(X_train , y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f27995",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dt_model.predict(X_test)\n",
    "dt_acc_score = accuracy_score(y_test , dt_pred)\n",
    "dt_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(dt_model , X_test , y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out FPR TPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93b0e3",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4ee01ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;minmaxscaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(n_estimators=500))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;minmaxscaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(n_estimators=500))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=500)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
       "                ('minmaxscaler', MinMaxScaler()),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(n_estimators=500))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest model\n",
    "\n",
    "rf_model = make_pipeline(SimpleImputer(strategy='mean') , MinMaxScaler() , RandomForestClassifier(n_estimators=500))\n",
    "rf_model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7dd93684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for test:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       639\n",
      "           1       0.00      0.00      0.00        85\n",
      "\n",
      "    accuracy                           0.88       724\n",
      "   macro avg       0.44      0.50      0.47       724\n",
      "weighted avg       0.78      0.88      0.83       724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_prob_train = rf_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_prob = rf_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Classification report for test:\\n',classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7788b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 9, 'max_features': 2, 'min_samples_leaf': 17, 'min_samples_split': 3, 'n_estimators': 17}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=1)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': sp_randint(5, 25),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': sp_randint(2, 10),\n",
    "    'min_samples_split': sp_randint(2, 20),\n",
    "    'min_samples_leaf': sp_randint(1, 20),\n",
    "    'max_features': sp_randint(2, 15)\n",
    "}\n",
    "\n",
    "rand_search_rfc = RandomizedSearchCV(rfc, param_distributions=params, cv=3, random_state=1)\n",
    "\n",
    "rand_search_rfc.fit(X, y)\n",
    "print(rand_search_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc0953b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for test:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       639\n",
      "           1       0.00      0.00      0.00        85\n",
      "\n",
      "    accuracy                           0.88       724\n",
      "   macro avg       0.44      0.50      0.47       724\n",
      "weighted avg       0.78      0.88      0.83       724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(**rand_search_rfc.best_params_)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = rfc.predict(X_train)\n",
    "y_prob_train = rfc.predict_proba(X_train)[:, 1]\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_prob = rfc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('Classification report for test:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b1cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
